---
title: AI Study
id: "20241121123223"
types:
  - ai
---

# ğŸ§  AI & LLM Mastery Roadmap
### 6-Month Guided Learning Journey â€” Theory, Practice, and Real-World Building

Welcome to the AI & LLM Learning Roadmap!  
This 6-month journey takes you from machine learning fundamentals to building and deploying advanced AI systems with tools like LangChain, OpenAI, and Hugging Face.  

Weâ€™ll learn together, meet weekly to discuss concepts, and tackle tutorials every two weeks. The course is split into **beginner** and **advanced** resources, so everyone can learn at their own depth â€” but all in sync.

---

## ğŸ—“ï¸ Month 1 â€” Machine Learning Foundations

### **Week 1 â€” What is Machine Learning?**
Letâ€™s kick things off with a solid understanding of what ML actually is. Youâ€™ll learn how algorithms find patterns in data, what makes AI â€œintelligent,â€ and the key differences between traditional software and ML systems.  
- ğŸ“˜ **Beginner Reading:** [Googleâ€™s ML Crash Course](https://developers.google.com/machine-learning/crash-course)  
- ğŸ”¬ **Advanced Reading:** [Deep Learning Book â€” Chapter 1](https://www.deeplearningbook.org/)  
- ğŸ§ª **Tutorial:** [Intro to ML in Python (Colab Notebook)](https://colab.research.google.com/github/ageron/handson-ml2/blob/master/01_the_machine_learning_landscape.ipynb)

---

### **Week 2 â€” Data, Features & Model Training**
This week is all about how machines â€œseeâ€ data. Youâ€™ll learn about datasets, feature engineering, data preprocessing, and training your first ML model.  
- ğŸ“˜ **Beginner Reading:** [Kaggleâ€™s Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)  
- ğŸ”¬ **Advanced Reading:** [Feature Engineering Principles â€” Google Developers](https://developers.google.com/machine-learning/data-prep/construct/transform/features)  
- ğŸ§ª **Tutorial:** [Scikit-Learn: Training Models Step-by-Step](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)

---

### **Week 3 â€” Supervised vs. Unsupervised Learning**
Explore how models learn from labeled (supervised) vs. unlabeled (unsupervised) data.  
- ğŸ“˜ **Beginner Reading:** [Supervised vs Unsupervised ML Explained (IBM)](https://www.ibm.com/blog/supervised-vs-unsupervised-learning/)  
- ğŸ”¬ **Advanced Reading:** [Clustering & Dimensionality Reduction â€” Stanford CS229 Notes](https://cs229.stanford.edu/notes2022fall/notes5.pdf)  
- ğŸ§ª **Tutorial:** [K-Means & PCA Hands-On Notebook](https://colab.research.google.com/github/ageron/handson-ml2/blob/master/08_dimensionality_reduction.ipynb)

---

### **Week 4 â€” Model Evaluation & Overfitting**
Youâ€™ll learn how to measure model accuracy, precision, recall, and prevent overfitting.  
- ğŸ“˜ **Beginner Reading:** [Model Evaluation Guide â€” Kaggle](https://www.kaggle.com/learn/model-validation)  
- ğŸ”¬ **Advanced Reading:** [Bias-Variance Tradeoff (CS229 Lecture Notes)](https://cs229.stanford.edu/notes2022fall/notes3.pdf)  
- ğŸ§ª **Tutorial:** [Evaluating ML Models â€” Python Walkthrough](https://www.datacamp.com/tutorial/understanding-bias-variance-tradeoff)

---

## âš™ï¸ Month 2 â€” Deep Learning & Neural Networks

### **Week 5 â€” Neural Network Basics**
Understand neurons, layers, activations, and how they combine to form a network.  
- ğŸ“˜ **Beginner Reading:** [Neural Networks Explained (3Blue1Brown Video)](https://www.youtube.com/watch?v=aircAruvnKk)  
- ğŸ”¬ **Advanced Reading:** [CS231n â€” Neural Networks Notes](https://cs231n.github.io/neural-networks-1/)  
- ğŸ§ª **Tutorial:** [Build Your First Neural Net in PyTorch](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)

---

### **Week 6 â€” Training Deep Models**
This week: gradient descent, backpropagation, and optimization algorithms like Adam and SGD.  
- ğŸ“˜ **Beginner Reading:** [Gradient Descent Visual Explanation (StatQuest)](https://www.youtube.com/watch?v=sDv4f4s2SB8)  
- ğŸ”¬ **Advanced Reading:** [Deep Learning Book â€” Optimization Chapters](https://www.deeplearningbook.org/contents/optimization.html)  
- ğŸ§ª **Tutorial:** [Train & Visualize Deep Nets (Colab)](https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb)

---

### **Week 7 â€” CNNs, RNNs & Sequence Models**
Explore how deep networks handle images and text â€” the stepping stones to LLMs.  
- ğŸ“˜ **Beginner Reading:** [Convolutional Neural Networks â€” Simplified](https://www.youtube.com/watch?v=YRhxdVk_sIs)  
- ğŸ”¬ **Advanced Reading:** [Understanding LSTMs (Colahâ€™s Blog)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
- ğŸ§ª **Tutorial:** [CNNs in PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)

---

### **Week 8 â€” Deep Learning Frameworks**
Get hands-on with TensorFlow, PyTorch, and Keras. Learn how they differ and where each excels.  
- ğŸ“˜ **Beginner Reading:** [PyTorch vs TensorFlow Overview (Datacamp)](https://www.datacamp.com/tutorial/pytorch-vs-tensorflow)  
- ğŸ”¬ **Advanced Reading:** [PyTorch Internals Explained](https://pytorch.org/blog/inside-pytorch/)  
- ğŸ§ª **Tutorial:** [TensorFlow Quickstart Colab](https://www.tensorflow.org/tutorials/quickstart/beginner)

---

## ğŸ”„ Month 3 â€” Transformers & LLM Fundamentals

### **Week 9 â€” Attention Mechanisms**
Discover how attention lets models focus on the right words or features when processing data.  
- ğŸ“˜ **Beginner Reading:** [Attention Explained (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)  
- ğŸ”¬ **Advanced Reading:** [â€œAttention Is All You Needâ€ Paper](https://arxiv.org/abs/1706.03762)  
- ğŸ§ª **Tutorial:** [Attention from Scratch (Colab)](https://colab.research.google.com/github/karpathy/nanoGPT/blob/master/playground.ipynb)

---

### **Week 10 â€” Transformer Architecture**
Weâ€™ll go deep into the architecture that powers GPT, BERT, and similar models.  
- ğŸ“˜ **Beginner Reading:** [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)  
- ğŸ”¬ **Advanced Reading:** [Annotated Transformer (Harvard NLP)](https://nlp.seas.harvard.edu/2018/04/03/attention.html)  
- ğŸ§ª **Tutorial:** [Build a Mini Transformer (Colab)](https://colab.research.google.com/github/karpathy/minGPT/blob/master/play_char.ipynb)

---

### **Week 11 â€” Tokenization & Embeddings**
Learn how text becomes numbers â€” and how meaning is encoded in embeddings.  
- ğŸ“˜ **Beginner Reading:** [How Tokenization Works (OpenAI)](https://help.openai.com/en/articles/6825453-how-does-tokenization-work)  
- ğŸ”¬ **Advanced Reading:** [Word2Vec and Beyond â€” The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)  
- ğŸ§ª **Tutorial:** [Sentence Embeddings (Hugging Face)](https://huggingface.co/course/chapter3)

---

### **Week 12 â€” Working with Pretrained Models**
Youâ€™ll use Hugging Face models for text generation, classification, and summarization.  
- ğŸ“˜ **Beginner Reading:** [Intro to Transformers (Hugging Face)](https://huggingface.co/course/chapter1)  
- ğŸ”¬ **Advanced Reading:** [Transformers Library Deep Dive](https://huggingface.co/docs/transformers/index)  
- ğŸ§ª **Tutorial:** [Hugging Face Text Generation Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/course/chapter1/transformers.ipynb)

---

## âš¡ Month 4 â€” LangChain, Agents & Retrieval Systems

### **Week 13 â€” LangChain Basics**
Build your first LLM-powered application using LangChain.  
- ğŸ“˜ **Beginner Reading:** [LangChain Official Intro](https://python.langchain.com/docs/get_started/introduction)  
- ğŸ”¬ **Advanced Reading:** [LangChain Architecture Explained (Medium)](https://towardsdatascience.com/a-deep-dive-into-langchain-architecture-2f8eebf6a63e)  
- ğŸ§ª **Tutorial:** [LangChain Crash Course (YouTube)](https://www.youtube.com/watch?v=lG7Uxts9SXs)

---

### **Week 14 â€” Prompt Engineering**
Learn how to craft better prompts and control model output.  
- ğŸ“˜ **Beginner Reading:** [Prompt Engineering Guide](https://www.promptingguide.ai/)  
- ğŸ”¬ **Advanced Reading:** [Chain-of-Thought Prompting Paper](https://arxiv.org/abs/2201.11903)  
- ğŸ§ª **Tutorial:** [OpenAI Playground Practice](https://platform.openai.com/playground)

---

### **Week 15 â€” Retrieval-Augmented Generation (RAG)**
Youâ€™ll learn how to connect LLMs with your own data using embeddings and vector databases.  
- ğŸ“˜ **Beginner Reading:** [RAG Concept Overview (LangChain Docs)](https://python.langchain.com/docs/use_cases/question_answering/)  
- ğŸ”¬ **Advanced Reading:** [In-Depth RAG Systems (Pinecone Blog)](https://www.pinecone.io/learn/retrieval-augmented-generation/)  
- ğŸ§ª **Tutorial:** [RAG from Scratch (YouTube)](https://www.youtube.com/watch?v=sVcwVQRHIc8)

---

### **Week 16 â€” Building AI Tools & Agents**
Understand how to make your LLMs take action â€” creating autonomous and tool-using agents.  
- ğŸ“˜ **Beginner Reading:** [LangChain Agents Intro](https://python.langchain.com/docs/modules/agents/)  
- ğŸ”¬ **Advanced Reading:** [ReAct Paper (Reason + Act)](https://arxiv.org/abs/2210.03629)  
- ğŸ§ª **Tutorial:** [Build an Agent in LangChain (YouTube)](https://www.youtube.com/watch?v=2xxziIWmaSA)

---

## ğŸ§© Month 5 â€” Scaling, Tuning & Deployment

### **Week 17 â€” Fine-Tuning & Custom Models**
Learn how to train your own small models for specialized tasks.  
- ğŸ“˜ **Beginner Reading:** [Fine-Tuning Explained (Hugging Face)](https://huggingface.co/docs/transformers/training)  
- ğŸ”¬ **Advanced Reading:** [Parameter-Efficient Fine-Tuning Paper](https://arxiv.org/abs/1909.00083)  
- ğŸ§ª **Tutorial:** [Fine-Tuning on Custom Data (Colab)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)

---

### **Week 18 â€” Evaluation & Metrics for LLMs**
Weâ€™ll cover perplexity, BLEU, and human feedback metrics.  
- ğŸ“˜ **Beginner Reading:** [Evaluating LLM Outputs (OpenAI Guide)](https://help.openai.com/en/articles/6825459-evaluating-ai-models)  
- ğŸ”¬ **Advanced Reading:** [Beyond BLEU Paper](https://aclanthology.org/W14-3346.pdf)  
- ğŸ§ª **Tutorial:** [LLM Evaluation Notebook (Colab)](https://colab.research.google.com/github/openai/evals/blob/main/examples/evals.ipynb)

---

### **Week 19 â€” Infrastructure & Scaling**
Discover how to deploy models at scale with Docker, Kubernetes, and cloud APIs.  
- ğŸ“˜ **Beginner Reading:** [Deploy Models with FastAPI](https://fastapi.tiangolo.com/deployment/)  
- ğŸ”¬ **Advanced Reading:** [LLM Inference Optimization (vLLM Blog)](https://vllm.ai/)  
- ğŸ§ª **Tutorial:** [Dockerize and Serve a Model (GitHub)](https://github.com/tiangolo/uvicorn-gunicorn-fastapi-docker)

---

### **Week 20 â€” Ethics & Safety in AI**
Understand bias, hallucinations, safety, and ethical deployment.  
- ğŸ“˜ **Beginner Reading:** [Ethics of AI (Google Responsible AI)](https://ai.google/responsibility/principles/)  
- ğŸ”¬ **Advanced Reading:** [RLHF Overview (OpenAI Paper Summary)](https://openai.com/research/learning-from-human-feedback)  
- ğŸ§ª **Tutorial:** [Bias Detection Exercise (Colab)](https://colab.research.google.com/github/google/responsible-ai-toolkit)

---

## ğŸš€ Month 6 â€” Capstone Projects & Future Horizons

### **Week 21â€“22 â€” Choose & Build Your Capstone**
Pick your final project: a LangChain app, fine-tuned model, or multi-agent system.  
- ğŸ“˜ **Project Ideas:** [Awesome LangChain Projects List](https://github.com/kyrolabs/awesome-langchain)  
- ğŸ§ª **Tutorial:** [Deploying LLM Apps to the Web (Streamlit)](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)

---

### **Week 23 â€” Testing & Demo Prep**
Polish your project, prepare your demo, and document your process.  
- ğŸ“˜ **Reading:** [Best Practices for ML Deployment](https://developers.google.com/machine-learning/guides/rules-of-ml)  
- ğŸ§ª **Tutorial:** [Model Card Creation (Hugging Face)](https://huggingface.co/docs/hub/model-cards)

---

### **Week 24 â€” Showcase & Next Steps**
Time to present! Reflect on what youâ€™ve built and where you want to go next.  
- ğŸ“˜ **Reading:** [AI Career Paths â€” Papers with Code Guide](https://paperswithcode.com/)  
- ğŸ”¬ **Advanced Reading:** [Scaling Laws in LLMs Paper](https://arxiv.org/abs/2001.08361)  

---

# ğŸ¯ Wrap-Up

By the end of this roadmap, youâ€™ll understand the **theory, architecture, and application** of modern AI systems â€” and have built projects that prove it. Whether youâ€™re an engineer, researcher, or product builder, youâ€™ll be ready to design intelligent systems that actually work in the real world.
